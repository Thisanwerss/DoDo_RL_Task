# ðŸ§© Purpose of Each File & Data Flow

This document explains the architecture and data flow of the CartPole RL project to help you **build smarter** and **debug easier**.

## ðŸ—‚ï¸ Root Files

### 1. `train.py` â€” **Main Training Runner**

**Purpose**: This script is the **entry point** for training the agent. It:
* Creates the environment
* Initializes the agent
* Runs the training loop with hyperparameters
* Saves the trained model

**Involves:**
* `config.py` (gets training config like episode count, epsilon, etc.)
* `environment.py` (for creating the CartPole environment)
* `agent.py` (where training logic lives)

**Flow**:
```
train.py â†’ config.py     # imports training settings
train.py â†’ environment.py # gets the Gym env
train.py â†’ agent.py      # creates and trains agent
agent.py â†’ utils.py      # logs or plots data
```

### 2. `evaluate.py` â€” **Evaluate the Trained Model**

**Purpose**: Runs the **trained agent** in the environment for a few episodes to see how well it performs.

**Involves:**
* Loads the saved model
* Uses same `environment.py` and `agent.py`

**Flow**:
```
evaluate.py â†’ config.py
evaluate.py â†’ environment.py
evaluate.py â†’ agent.py
```

### 3. `config.py` â€” **Central Hyperparameter File**

**Purpose**: Holds all the constants and hyperparameters. Examples:
* Learning rate
* Discount factor
* Epsilon decay
* Model save path

**Used by**:
* `train.py`
* `evaluate.py`

## ðŸ§  `src/` Folder â€” Core Logic

### 4. `agent.py` â€” **RL Agent Logic**

**Purpose**: The brain of the project. This file contains:
* The Q-learning logic (Q-table update rule)
* The exploration strategy (epsilon-greedy)
* `train()` and `evaluate()` methods
* `save()` and `load()` functions for models

**Used by**:
* `train.py` (to train)
* `evaluate.py` (to test)

### 5. `environment.py` â€” **Environment Creation Helper**

**Purpose**: A simple utility to initialize and return the OpenAI Gym (Gymnasium) environment. It handles both:
* `render_mode="human"` for watching during evaluation
* Basic wrappers (if needed later)

**Used by**:
* `train.py`
* `evaluate.py`

### 6. `utils.py` â€” **Utility Functions**

**Purpose**: A place for helper tools that don't belong in core logic, like:
* Plotting training reward
* Logging to console or file
* Saving performance graphs

## ðŸ“˜ `docs/` Folder â€” Markdown Lessons

Each `.md` file teaches you theory or explains parts of the codebase. Great for learning & for showcasing in your portfolio.

Examples:
* `00_intro_to_rl.md` â€” what is RL?
* `04_training_walkthrough.md` â€” line-by-line `train.py` breakdown

## ðŸ“ `models/` Folder

Stores the `.pth` file (PyTorch-style model saving) or even a `.npy` Q-table later.

## ðŸ§ª `notebooks/`

You can test ideas or visualize rewards with plots interactively using Jupyter Notebooks here. Optional but great for learning.

## ðŸ” Data Flow Diagram

```mermaid
flowchart TD
    %% Main Training Flow
    subgraph Training ["Training Process"]
        train[train.py] --> config[config.py]
        train --> env[environment.py]
        train --> agent[agent.py]
        agent --> utils[utils.py]
        agent --> |saves model| models[(models/cartpole_model.pth)]
    end
    
    %% Evaluation Flow
    subgraph Evaluation ["Evaluation Process"]
        eval[evaluate.py] --> config
        eval --> env
        eval --> agent
        models --> |loads model| agent
    end
    
    %% Documentation
    subgraph Documentation ["Documentation"]
        docs[docs/*.md] -.-> |explains| Training
        docs -.-> |explains| Evaluation
        notebooks[notebooks/*.ipynb] -.-> |visualizes results| Training
    end
    
    %% Visual styling
    classDef coreFiles fill:#f9f,stroke:#333,stroke-width:2px;
    classDef utilFiles fill:#bbf,stroke:#333,stroke-width:1px;
    classDef dataFiles fill:#bfb,stroke:#333,stroke-width:1px;
    
    class train,eval,agent coreFiles;
    class config,env,utils utilFiles;
    class models,notebooks dataFiles;
```

## âœ… Summary Table

| File | What it Does | Used By |
|------|--------------|---------|
| `train.py` | Runs training loop | You run it to train |
| `evaluate.py` | Runs evaluation loop | You run it to test |
| `config.py` | Stores hyperparameters | All main scripts |
| `agent.py` | Q-learning logic + save/load | `train`, `evaluate` |
| `environment.py` | Returns the Gym env | `train`, `evaluate` |
| `utils.py` | Extra tools (plotting, logging) | `agent.py` (optional) |
| `models/` | Stores saved model | Created after training |
| `docs/` | Markdown lessons | For your learning |

## ðŸ“Š Q-Learning Data Flow

```mermaid
flowchart LR
    subgraph "Q-Learning Process"
        direction TB
        A[Agent observes state] --> B[Choose action using Îµ-greedy]
        B --> C[Take action in environment]
        C --> D[Get reward & next state]
        D --> E[Update Q-table using Bellman equation]
        E --> F[Update state]
        F --> A
    end
    
    subgraph "Îµ-greedy Strategy"
        direction TB
        G[Generate random number] --> H{Less than Îµ?}
        H -->|Yes| I[Random action]
        H -->|No| J[Best Q-value action]
    end
```

## ðŸ”„ Training Loop Flow

```mermaid
sequenceDiagram
    participant Train as train.py
    participant Config as config.py
    participant Env as environment.py
    participant Agent as agent.py
    participant Utils as utils.py
    participant Models as models/
    
    Train->>Config: Get hyperparameters
    Train->>Env: Create environment
    Train->>Agent: Initialize agent
    
    loop For each episode
        Train->>Agent: Run episode
        Agent->>Env: Reset environment
        
        loop Until episode ends
            Agent->>Env: Take action
            Env->>Agent: Return next_state, reward, done
            Agent->>Agent: Update Q-values
        end
        
        Agent->>Utils: Log episode stats
    end
    
    Agent->>Models: Save trained model
    Utils->>Utils: Generate performance plots
```
